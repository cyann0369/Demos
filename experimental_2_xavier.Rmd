---
title: "Demos Data Analysis Notebook"
output: html_notebook
---
# Demos Data Project

## Imports

```{r}
# plot graphs
#library(ggplot2)

#library(caret)  
#library(stats) # linear reg

# package for correlation analysis.
#library('corrr')

# plot correlations
#library(ggcorrplot)

# Lib for Clustering
library(cluster)
```


```{r, include=FALSE}
source("data_analysis.R", local = knitr::knit_global())
```


## Data Exploration

```{r}
data <- load_data("data/data_abs.xlsx")
glimpse(data)
```
### Checking Missing Values

```{r}
colSums(is.na(data))
```

### Visualizing the Data

```{r}
summary(data)

```

```{r}
generate_reactable_table(data)
```

## Data Analysis


```{r}
draw_correlation_heatmap(data)
```
```{r}

draw_scatterplot(data,"Salairemoy","NonDiplome")
draw_scatterplot(data,"TxPauv","txcho")
draw_scatterplot(data,"TxPauv","txabs")
```

```{r}
library(car)
columns <- c("HLM","Salairemoy","TxPauv","NonDiplome","txcho","txabs")
subdata <- data[columns]
scatterplotMatrix(subdata, smooth = FALSE, cex=.2,pch=19,main="Pair plots", regLine = (list(method=lm, lwd=1,col="red")))
```


```{r}
subdata
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the data
# calculate correlation matrix
correlationMatrix <- cor(subdata)
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)


# ensure results are repeatable

# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(txabs~., data=subdata, method="lm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```


### Analyse Univariée

```{r}
summary(data$HLM)
data
```


### Analyse Bivarié


##

#### AJOUTER PIECHART PROPORTION DE CHAQUE PROFESSION


## Preprocessing

```{r}
data
```

### Scaling
```{r}
# List of columns to normalize
columns_to_normalize <- c("HLM", "Salairemoy", "TxPauv", "NonDiplome", "txcho")

# Apply preProcess to the desired columns
df_scaled <- min_max_scaling(data, "txabs", columns_to_normalize, FALSE)

head(df_scaled)
```
### Train-Test Split

```{r}
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df_scaled$txabs, p = 0.8, list = FALSE)
train_data <- df_scaled[train_index, ]
test_data <- df_scaled[-train_index, ]

X_train <- select(train_data, -txabs)  # Select all columns except txabs
y_train <- train_data$txabs  # Target variable

X_test <- select(test_data, -txabs)  # Select all columns except txabs
y_test <- test_data$txabs  # Target variable


```

```{r}
X_train
```


```{r}
colnames(train_data)

```
```{r}
subdata<-data[!rownames(data) %in% c("Corse-du-Sud","Haute-Corse","Moselle","Haute-Savoie"),]
```


## Linear Regression

```{r}
# Fit the linear regression model
#fit <- lm(txabs ~ HLM + Salairemoy + TxPauv + NonDiplome + txcho, data = df_scaled)

fit <- lm(txabs ~ I(TxPauv^1) , data = subdata)

summary(fit)
par(mfrow=c(2,2))
plot(fit,pch=20)
library(car)
#vif(fit)



# Make predictions on the test set
#predictions <- predict(fit, newdata = test_data)

# Evaluate the model (e.g., using RMSE)
#rmse <- sqrt(mean((test_data$txabs - predictions)^2))
#cat("Root Mean Squared Error (RMSE):", rmse, "\n")
```
```{r}
df_scaled[-1]
```




```{r}
# Determine the optimal number of clusters using the Elbow Method
wss <- numeric(10) # to store within-cluster sum of squares for each cluster number

for (k in 1:10) {
  #kmeans_model <- kmeans(X_train, centers = k, nstart = 5)
  kmeans_model <- kmeans(df_scaled[-1], centers = k, nstart = 5)
  wss[k] <- kmeans_model$tot.withinss
}

# Plot the Elbow Curve
elbow_data <- data.frame(Clusters = 1:10, WSS = wss)
ggplot(elbow_data, aes(x = Clusters, y = WSS)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow Method for Optimal Clusters",
       x = "Number of Clusters",
       y = "Total Within-Cluster Sum of Squares")

# Step 3: Fit the K-means model with the chosen number of clusters
optimal_clusters <- 3 # Change this based on the elbow plot
#kmeans_result <- kmeans(X_train, centers = optimal_clusters, nstart = 5)
kmeans_result <- kmeans(df_scaled[-1], centers = optimal_clusters, nstart = 5)

# View the results
print(kmeans_result)

# Add the cluster assignments to the original data
df_scaled$Cluster <- kmeans_result$cluster

```

```{r}
# Step 1: Calculate the Gap Statistic
#
#theGap <- clusGap(X_train, FUNcluster = pam, K.max = k)
theGap <- clusGap(df_scaled[-1], FUNcluster = pam, K.max = k)

# Step 2: Convert Gap Statistic results to a data frame
gapDF <- as.data.frame(theGap$Tab)

# Step 3: Plot logW curves
ggplot(gapDF, aes(x=1:nrow(gapDF))) +
    geom_line(aes(y=logW), color="blue") +
    geom_point(aes(y=logW), color="blue") +
    geom_line(aes(y=E.logW), color="green") +
    geom_point(aes(y=E.logW), color="green") +
    labs(x="Number of Clusters", y="Log W Values", title="Log W Curves")

# Step 4: Plot the gap curve
ggplot(gapDF, aes(x=1:nrow(gapDF))) +
    geom_line(aes(y=gap), color="red") +
    geom_point(aes(y=gap), color="red") +
    geom_errorbar(aes(ymin=gap - SE.sim, ymax=gap + SE.sim), color="red") +
    labs(x="Number of Clusters", y="Gap", title="Gap Curve")

```


```{r}
data$Cluster = as.character(df_scaled$Cluster)
```


```{r}
for (col1 in columns_to_normalize){
  for (col2 in columns_to_normalize){
    if (col1!=col2){
      g <- draw_clusters(data,col1,col2)
      print(g)
    }
  }
}
```


```{r}
for (col1 in columns_to_normalize){
      g <- draw_clusters(data,col1,"txabs")
      print(g)
}
```

```{r}
library(Rtsne)
subdf <- df_scaled[,-1]
subdf <- subdf[-6]
tsne_out <- Rtsne(subdf)

fix_class <- function(x){
  if (x<=12){
    return("A")
  }
  else if (x<=20){
    return("B")
  }
  return("C")
}

# Conversion of matrix to dataframe
tsne_plot <- data.frame(x = tsne_out$Y[,1], 
                        y = tsne_out$Y[,2],
                        color = as.character(df_scaled$Cluster),
                        info = sapply(data$HLM,fix_class))
                        
 
# Plotting the plot using ggplot() functionà
set.seed(123)

ggplot(tsne_plot) + 
  geom_point(aes(x=x,y=y,color=color))
ggplot(tsne_plot) + 
  geom_point(aes(x=x,y=y,color=info))

```

```{r}


# Library
library(fmsb)
 
# Create data: note in High school for several students
#set.seed(99)
data <- as.data.frame(matrix( sample( 0:20 , 15 , replace=F) , ncol=5))
colnames(data) <- c("math" , "english" , "biology" , "music" , "R-coding" )
rownames(data) <- paste("mister" , letters[1:3] , sep="-")
 
# To use the fmsb package, I have to add 2 lines to the dataframe: the max and min of each variable to show on the plot!
data <- rbind(rep(20,5) , rep(0,5) , data)
 
# plot with default options:
radarchart(data)
data

kmeans_df <- rbind(min=apply(kmeans_df,2,min),
                   max=apply(kmeans_df,2,max),
                   kmeans_result$centers
                   )

kmeans_df <- data.frame(kmeans_df)
kmeans_df
colors_border=c( rgb(0.2,0.5,0.5,0.9), 
                 rgb(0.8,0.2,0.5,0.9), 
                 rgb(0.7,0.5,0.1,0.9) 
                 )
colors_in=c( rgb(0.2,0.5,0.5,0.4), 
             rgb(0.8,0.2,0.5,0.4) , 
             rgb(0.7,0.5,0.1,0.4) )


radarchart(kmeans_df, 
           title="Position des centroïdes",
           pcol=colors_border,
           pfcol=colors_in)
legend(x=2, y=1, legend = rownames(kmeans_df[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)

```


```{r fwidth=6, fig.height=12}
d.subdf <- dist(subdf)
cah.ward <- hclust(d.subdf,method="ward.D2")
plot(cah.ward)
rect.hclust(cah.ward,k=3)
```
```{r fwidth=6, fig.height=12}
plot(cah.ward)
rect.hclust(cah.ward,k=3)
groupes.cah <- cutree(cah.ward,k=3)
groupes.cah
```


```{r}
library(cluster)

silhouette_score_mean <- function(k){
  km <- kmeans(subdf, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(subdf))
  mean(ss[, 3])
}

k <- 2:10
avg_sil <- sapply(k,silhouette_score_mean)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)

```

```{r}
library(factoextra)
fviz_nbclust(subdf, kmeans, method='silhouette')
```

```{r}
my_silhouette_score <- function(k){
  km <- kmeans(subdf, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(subdf))
  ss
}

#k <- 2:10n
for (k in 2:10){
  sil <- my_silhouette_score(k)
  plot(fviz_silhouette(sil))
}

```

